{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afb5a23-7f96-4c70-84d5-12c7e2e25893",
      "metadata": {
        "id": "3afb5a23-7f96-4c70-84d5-12c7e2e25893",
        "outputId": "a2376f00-999b-4708-a42b-f542dd94de12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: /physical_device:GPU:0, Details: {'device_name': 'NVIDIA GeForce RTX 4050 Laptop GPU', 'compute_capability': (8, 9)}\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# List physical GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        details = tf.config.experimental.get_device_details(gpu)\n",
        "        print(f\"Device: {gpu.name}, Details: {details}\")\n",
        "else:\n",
        "    print(\"No GPU found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9cb8f4f-64ac-4138-a4ac-5e145fc8bbf5",
      "metadata": {
        "id": "e9cb8f4f-64ac-4138-a4ac-5e145fc8bbf5",
        "outputId": "f1897d63-def7-44ec-a1d0-5c259dfe0792"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Original Data Head"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>dssp3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n",
              "      <td>CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
              "      <td>CCHHHHHHHHHCCEEEEEECCCCCEEEECCEEEECCCCCCCCHHHH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n",
              "      <td>CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTAAKSELDKAIGR...</td>\n",
              "      <td>CCHHHHHHHHHCCEEEEEECCCCCEEEECCEECCHHHHHHHHHHCC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
              "      <td>CCHHHHHHHHCCCCCECEECCCCCEEECCCEEEECCCCCCHHHHHH...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input  \\\n",
              "0  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n",
              "1  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
              "2  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n",
              "3  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTAAKSELDKAIGR...   \n",
              "4  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
              "\n",
              "                                               dssp3  \n",
              "0  CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...  \n",
              "1  CCHHHHHHHHHCCEEEEEECCCCCEEEECCEEEECCCCCCCCHHHH...  \n",
              "2  CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...  \n",
              "3  CCHHHHHHHHHCCEEEEEECCCCCEEEECCEECCHHHHHHHHHHCC...  \n",
              "4  CCHHHHHHHHCCCCCECEECCCCCEEECCCEEEECCCCCCHHHHHH...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "## Mappings"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Amino Acid Dictionary:** {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Secondary Structure Dictionary:** {'C': 1, 'H': 2, 'E': 3}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "## Data After Processing (First 5 Rows)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>dssp3</th>\n",
              "      <th>processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n",
              "      <td>CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...</td>\n",
              "      <td>([11, 18, 10, 16, 4, 6, 4, 19, 14, 10, 18, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
              "      <td>CCHHHHHHHHHCCEEEEEECCCCCEEEECCEEEECCCCCCCCHHHH...</td>\n",
              "      <td>([11, 12, 8, 5, 4, 11, 10, 15, 8, 3, 4, 6, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...</td>\n",
              "      <td>CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...</td>\n",
              "      <td>([11, 18, 10, 16, 4, 6, 4, 19, 14, 10, 18, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTAAKSELDKAIGR...</td>\n",
              "      <td>CCHHHHHHHHHCCEEEEEECCCCCEEEECCEECCHHHHHHHHHHCC...</td>\n",
              "      <td>([11, 12, 8, 5, 4, 11, 10, 15, 8, 3, 4, 6, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...</td>\n",
              "      <td>CCHHHHHHHHCCCCCECEECCCCCEEECCCEEEECCCCCCHHHHHH...</td>\n",
              "      <td>([11, 12, 8, 5, 4, 11, 10, 15, 8, 3, 4, 6, 10,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input  \\\n",
              "0  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n",
              "1  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
              "2  MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDR...   \n",
              "3  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTAAKSELDKAIGR...   \n",
              "4  MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSEL...   \n",
              "\n",
              "                                               dssp3  \\\n",
              "0  CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...   \n",
              "1  CCHHHHHHHHHCCEEEEEECCCCCEEEECCEEEECCCCCCCCHHHH...   \n",
              "2  CCCCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCHHHHHHCCC...   \n",
              "3  CCHHHHHHHHHCCEEEEEECCCCCEEEECCEECCHHHHHHHHHHCC...   \n",
              "4  CCHHHHHHHHCCCCCECEECCCCCEEECCCEEEECCCCCCHHHHHH...   \n",
              "\n",
              "                                           processed  \n",
              "0  ([11, 18, 10, 16, 4, 6, 4, 19, 14, 10, 18, 10,...  \n",
              "1  ([11, 12, 8, 5, 4, 11, 10, 15, 8, 3, 4, 6, 10,...  \n",
              "2  ([11, 18, 10, 16, 4, 6, 4, 19, 14, 10, 18, 10,...  \n",
              "3  ([11, 12, 8, 5, 4, 11, 10, 15, 8, 3, 4, 6, 10,...  \n",
              "4  ([11, 12, 8, 5, 4, 11, 10, 15, 8, 3, 4, 6, 10,...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "## Amino Acid Sequences Array (First 5 Entries)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([[11, 18, 10, ...,  0,  0,  0],\n",
              "       [11, 12,  8, ...,  0,  0,  0],\n",
              "       [11, 18, 10, ...,  0,  0,  0],\n",
              "       [11, 12,  8, ...,  0,  0,  0],\n",
              "       [11, 12,  8, ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "## Secondary Structure Labels Array (First 5 Entries)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "## Data Split Shapes"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**X_train shape:** (87342, 512)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**y_train shape:** (87342, 512, 4)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**X_val shape:** (12727, 512)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**y_val shape:** (12727, 512, 4)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**X_test shape:** (24706, 512)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**y_test shape:** (24706, 512, 4)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, GRU, TimeDistributed, Dense, Dropout, Input\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Load and Prepare Data\n",
        "# ---------------------------\n",
        "data_path = 'list_merged_no_X_no_dups_cleaned_final.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "display(Markdown(\"## Original Data Head\"))\n",
        "display(df.head())\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Configuration and Mappings\n",
        "# ---------------------------\n",
        "vocab_size = 21  # 20 amino acids + padding\n",
        "num_classes = 4  # 3 secondary structure classes + padding\n",
        "max_seq_len = 512\n",
        "embedding_dim = 512\n",
        "dropout_rate = 0.6\n",
        "l2_lambda = 0.01\n",
        "\n",
        "# Mapping dictionaries\n",
        "aa_dict = {aa: idx for idx, aa in enumerate('ACDEFGHIKLMNPQRSTVWY', 1)}\n",
        "ss_dict = {'C': 1, 'H': 2, 'E': 3}  # 0 reserved for padding\n",
        "\n",
        "display(Markdown(\"## Mappings\"))\n",
        "display(Markdown(f\"**Amino Acid Dictionary:** {aa_dict}\"))\n",
        "display(Markdown(f\"**Secondary Structure Dictionary:** {ss_dict}\"))\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Data Preprocessing\n",
        "# ---------------------------\n",
        "def process_sequence(row):\n",
        "    aa_padded = row['input'].ljust(max_seq_len, '0')[:max_seq_len]\n",
        "    ss_padded = row['dssp3'].ljust(max_seq_len, '0')[:max_seq_len]\n",
        "\n",
        "    aa_encoded = [aa_dict.get(c, 0) for c in aa_padded]\n",
        "    ss_encoded = [ss_dict.get(c, 0) for c in ss_padded]\n",
        "\n",
        "    ss_one_hot = tf.keras.utils.to_categorical(ss_encoded, num_classes=num_classes)\n",
        "\n",
        "    return aa_encoded, ss_one_hot\n",
        "\n",
        "df['processed'] = df.apply(process_sequence, axis=1)\n",
        "\n",
        "display(Markdown(\"## Data After Processing (First 5 Rows)\"))\n",
        "display(df[['input', 'dssp3', 'processed']].head())\n",
        "\n",
        "# Extract sequences and labels\n",
        "aa_sequences = np.array([x[0] for x in df['processed']])\n",
        "ss_labels = np.array([x[1] for x in df['processed']])\n",
        "\n",
        "display(Markdown(\"## Amino Acid Sequences Array (First 5 Entries)\"))\n",
        "display(aa_sequences[:5])\n",
        "display(Markdown(\"## Secondary Structure Labels Array (First 5 Entries)\"))\n",
        "display(ss_labels[:5])\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Train/Validation/Test Split\n",
        "# ---------------------------\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(aa_sequences, ss_labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
        "\n",
        "display(Markdown(\"## Data Split Shapes\"))\n",
        "display(Markdown(f\"**X_train shape:** {X_train.shape}\"))\n",
        "display(Markdown(f\"**y_train shape:** {y_train.shape}\"))\n",
        "display(Markdown(f\"**X_val shape:** {X_val.shape}\"))\n",
        "display(Markdown(f\"**y_val shape:** {y_val.shape}\"))\n",
        "display(Markdown(f\"**X_test shape:** {X_test.shape}\"))\n",
        "display(Markdown(f\"**y_test shape:** {y_test.shape}\"))\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: Custom Metrics and Callbacks\n",
        "# ---------------------------\n",
        "class LearningRateTracker(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)\n",
        "        super().on_epoch_end(epoch, logs)\n",
        "\n",
        "def masked_categorical_crossentropy(y_true, y_pred):\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(tf.argmax(y_true, axis=-1), 0), tf.float32)\n",
        "    return tf.reduce_sum(loss * mask) / tf.maximum(tf.reduce_sum(mask), 1)\n",
        "\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    y_true_class = tf.argmax(y_true, axis=-1)\n",
        "    y_pred_class = tf.argmax(y_pred, axis=-1)\n",
        "    mask = tf.cast(tf.not_equal(y_true_class, 0), tf.float32)\n",
        "    matches = tf.cast(tf.equal(y_true_class, y_pred_class), tf.float32) * mask\n",
        "    return tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='best_model_kernel_5.h5',         # Path where the model will be saved\n",
        "    monitor='val_masked_accuracy',     # Metric to monitor\n",
        "    mode='max',                        # We want to maximize the accuracy\n",
        "    save_best_only=True,               # Only save if the metric improves\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 6: Build the Model\n",
        "# ---------------------------\n",
        "model = Sequential([\n",
        "    Input(shape=(max_seq_len,), dtype='int32'),\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
        "    Conv1D(512, 5, activation='relu', padding='same'),\n",
        "    Conv1D(512, 5, activation='relu', padding='same'),\n",
        "    Conv1D(512, 5, activation='relu', padding='same'),\n",
        "    Bidirectional(LSTM(512, return_sequences=True, kernel_regularizer=l2(l2_lambda))),\n",
        "    Dropout(dropout_rate),\n",
        "    Bidirectional(GRU(512, return_sequences=True, kernel_regularizer=l2(l2_lambda))),\n",
        "    Dropout(dropout_rate),\n",
        "    TimeDistributed(Dense(128, activation='relu')),\n",
        "    Dropout(dropout_rate),\n",
        "    TimeDistributed(Dense(64, activation='relu')),\n",
        "    Dropout(dropout_rate),\n",
        "    TimeDistributed(Dense(num_classes, activation='softmax'))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf94e19-21e0-4c84-9434-02403e946e00",
      "metadata": {
        "id": "5cf94e19-21e0-4c84-9434-02403e946e00",
        "outputId": "9acd768c-aecc-474b-aa35-8f4f5a76510c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 512, 512)          10752     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 512, 512)          1311232   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 512, 512)          1311232   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 512, 512)          1311232   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 512, 1024)        4198400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512, 1024)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 512, 1024)        4724736   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512, 1024)         0         \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 512, 128)         131200    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 512, 64)          8256      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512, 64)           0         \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 512, 4)           260       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,007,300\n",
            "Trainable params: 13,007,300\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 3.0392 - masked_accuracy: 0.6641      \n",
            "Epoch 1: val_masked_accuracy improved from -inf to 0.70922, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 954s 345ms/step - loss: 3.0392 - masked_accuracy: 0.6641 - val_loss: 0.6936 - val_masked_accuracy: 0.7092 - lr: 1.0000e-04\n",
            "Epoch 2/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.7040 - masked_accuracy: 0.7132   \n",
            "Epoch 2: val_masked_accuracy improved from 0.70922 to 0.72754, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 920s 337ms/step - loss: 0.7040 - masked_accuracy: 0.7132 - val_loss: 0.6477 - val_masked_accuracy: 0.7275 - lr: 1.0000e-04\n",
            "Epoch 3/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.6649 - masked_accuracy: 0.7300   \n",
            "Epoch 3: val_masked_accuracy improved from 0.72754 to 0.73539, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 925s 339ms/step - loss: 0.6649 - masked_accuracy: 0.7300 - val_loss: 0.6324 - val_masked_accuracy: 0.7354 - lr: 1.0000e-04\n",
            "Epoch 4/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.6335 - masked_accuracy: 0.7438   \n",
            "Epoch 4: val_masked_accuracy improved from 0.73539 to 0.75414, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 928s 340ms/step - loss: 0.6335 - masked_accuracy: 0.7438 - val_loss: 0.5918 - val_masked_accuracy: 0.7541 - lr: 1.0000e-04\n",
            "Epoch 5/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.6059 - masked_accuracy: 0.7561   \n",
            "Epoch 5: val_masked_accuracy improved from 0.75414 to 0.76290, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 948s 347ms/step - loss: 0.6059 - masked_accuracy: 0.7561 - val_loss: 0.5721 - val_masked_accuracy: 0.7629 - lr: 1.0000e-04\n",
            "Epoch 6/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.5803 - masked_accuracy: 0.7676   \n",
            "Epoch 6: val_masked_accuracy improved from 0.76290 to 0.76753, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.5803 - masked_accuracy: 0.7676 - val_loss: 0.5589 - val_masked_accuracy: 0.7675 - lr: 1.0000e-04\n",
            "Epoch 7/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.5576 - masked_accuracy: 0.7776   \n",
            "Epoch 7: val_masked_accuracy improved from 0.76753 to 0.77759, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 919s 336ms/step - loss: 0.5576 - masked_accuracy: 0.7776 - val_loss: 0.5445 - val_masked_accuracy: 0.7776 - lr: 1.0000e-04\n",
            "Epoch 8/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.5359 - masked_accuracy: 0.7873   \n",
            "Epoch 8: val_masked_accuracy improved from 0.77759 to 0.78632, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.5359 - masked_accuracy: 0.7873 - val_loss: 0.5249 - val_masked_accuracy: 0.7863 - lr: 1.0000e-04\n",
            "Epoch 9/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.5158 - masked_accuracy: 0.7961   \n",
            "Epoch 9: val_masked_accuracy improved from 0.78632 to 0.79463, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.5158 - masked_accuracy: 0.7961 - val_loss: 0.5055 - val_masked_accuracy: 0.7946 - lr: 1.0000e-04\n",
            "Epoch 10/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4985 - masked_accuracy: 0.8037   \n",
            "Epoch 10: val_masked_accuracy improved from 0.79463 to 0.80022, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.4985 - masked_accuracy: 0.8037 - val_loss: 0.4951 - val_masked_accuracy: 0.8002 - lr: 1.0000e-04\n",
            "Epoch 11/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4816 - masked_accuracy: 0.8108   \n",
            "Epoch 11: val_masked_accuracy did not improve from 0.80022\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.4816 - masked_accuracy: 0.8108 - val_loss: 0.5151 - val_masked_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 12/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4673 - masked_accuracy: 0.8172   \n",
            "Epoch 12: val_masked_accuracy improved from 0.80022 to 0.81077, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.4673 - masked_accuracy: 0.8172 - val_loss: 0.4720 - val_masked_accuracy: 0.8108 - lr: 1.0000e-04\n",
            "Epoch 13/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4524 - masked_accuracy: 0.8234   \n",
            "Epoch 13: val_masked_accuracy improved from 0.81077 to 0.81205, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 921s 337ms/step - loss: 0.4524 - masked_accuracy: 0.8234 - val_loss: 0.4672 - val_masked_accuracy: 0.8121 - lr: 1.0000e-04\n",
            "Epoch 14/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4396 - masked_accuracy: 0.8290   \n",
            "Epoch 14: val_masked_accuracy improved from 0.81205 to 0.82045, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 916s 335ms/step - loss: 0.4396 - masked_accuracy: 0.8290 - val_loss: 0.4524 - val_masked_accuracy: 0.8205 - lr: 1.0000e-04\n",
            "Epoch 15/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4274 - masked_accuracy: 0.8341   \n",
            "Epoch 15: val_masked_accuracy did not improve from 0.82045\n",
            "2730/2730 [==============================] - 915s 335ms/step - loss: 0.4274 - masked_accuracy: 0.8341 - val_loss: 0.4605 - val_masked_accuracy: 0.8178 - lr: 1.0000e-04\n",
            "Epoch 16/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4166 - masked_accuracy: 0.8387   \n",
            "Epoch 16: val_masked_accuracy improved from 0.82045 to 0.82445, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 914s 335ms/step - loss: 0.4166 - masked_accuracy: 0.8387 - val_loss: 0.4436 - val_masked_accuracy: 0.8245 - lr: 1.0000e-04\n",
            "Epoch 17/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.4058 - masked_accuracy: 0.8433   \n",
            "Epoch 17: val_masked_accuracy improved from 0.82445 to 0.82908, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 1159s 425ms/step - loss: 0.4058 - masked_accuracy: 0.8433 - val_loss: 0.4359 - val_masked_accuracy: 0.8291 - lr: 1.0000e-04\n",
            "Epoch 18/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3962 - masked_accuracy: 0.8474   \n",
            "Epoch 18: val_masked_accuracy improved from 0.82908 to 0.83225, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 1194s 437ms/step - loss: 0.3962 - masked_accuracy: 0.8474 - val_loss: 0.4336 - val_masked_accuracy: 0.8322 - lr: 1.0000e-04\n",
            "Epoch 19/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3869 - masked_accuracy: 0.8512   \n",
            "Epoch 19: val_masked_accuracy improved from 0.83225 to 0.83339, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 1305s 478ms/step - loss: 0.3869 - masked_accuracy: 0.8512 - val_loss: 0.4352 - val_masked_accuracy: 0.8334 - lr: 1.0000e-04\n",
            "Epoch 20/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3782 - masked_accuracy: 0.8549   \n",
            "Epoch 20: val_masked_accuracy improved from 0.83339 to 0.83450, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 973s 356ms/step - loss: 0.3782 - masked_accuracy: 0.8549 - val_loss: 0.4395 - val_masked_accuracy: 0.8345 - lr: 1.0000e-04\n",
            "Epoch 21/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3707 - masked_accuracy: 0.8581   \n",
            "Epoch 21: val_masked_accuracy improved from 0.83450 to 0.83860, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 926s 339ms/step - loss: 0.3707 - masked_accuracy: 0.8581 - val_loss: 0.4240 - val_masked_accuracy: 0.8386 - lr: 1.0000e-04\n",
            "Epoch 22/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3626 - masked_accuracy: 0.8614   \n",
            "Epoch 22: val_masked_accuracy improved from 0.83860 to 0.83953, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 920s 337ms/step - loss: 0.3626 - masked_accuracy: 0.8614 - val_loss: 0.4197 - val_masked_accuracy: 0.8395 - lr: 1.0000e-04\n",
            "Epoch 23/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3559 - masked_accuracy: 0.8643   \n",
            "Epoch 23: val_masked_accuracy improved from 0.83953 to 0.84216, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 914s 335ms/step - loss: 0.3559 - masked_accuracy: 0.8643 - val_loss: 0.4201 - val_masked_accuracy: 0.8422 - lr: 1.0000e-04\n",
            "Epoch 24/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3490 - masked_accuracy: 0.8673   \n",
            "Epoch 24: val_masked_accuracy improved from 0.84216 to 0.84386, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 915s 335ms/step - loss: 0.3490 - masked_accuracy: 0.8673 - val_loss: 0.4131 - val_masked_accuracy: 0.8439 - lr: 1.0000e-04\n",
            "Epoch 25/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3427 - masked_accuracy: 0.8699   \n",
            "Epoch 25: val_masked_accuracy improved from 0.84386 to 0.84556, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 913s 335ms/step - loss: 0.3427 - masked_accuracy: 0.8699 - val_loss: 0.4093 - val_masked_accuracy: 0.8456 - lr: 1.0000e-04\n",
            "Epoch 26/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3363 - masked_accuracy: 0.8726   \n",
            "Epoch 26: val_masked_accuracy improved from 0.84556 to 0.84744, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 915s 335ms/step - loss: 0.3363 - masked_accuracy: 0.8726 - val_loss: 0.4039 - val_masked_accuracy: 0.8474 - lr: 1.0000e-04\n",
            "Epoch 27/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3313 - masked_accuracy: 0.8747   \n",
            "Epoch 27: val_masked_accuracy improved from 0.84744 to 0.84853, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 915s 335ms/step - loss: 0.3313 - masked_accuracy: 0.8747 - val_loss: 0.4060 - val_masked_accuracy: 0.8485 - lr: 1.0000e-04\n",
            "Epoch 28/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3261 - masked_accuracy: 0.8769   \n",
            "Epoch 28: val_masked_accuracy improved from 0.84853 to 0.85024, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.3261 - masked_accuracy: 0.8769 - val_loss: 0.4105 - val_masked_accuracy: 0.8502 - lr: 1.0000e-04\n",
            "Epoch 29/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3204 - masked_accuracy: 0.8792   \n",
            "Epoch 29: val_masked_accuracy improved from 0.85024 to 0.85231, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 916s 335ms/step - loss: 0.3204 - masked_accuracy: 0.8792 - val_loss: 0.4089 - val_masked_accuracy: 0.8523 - lr: 1.0000e-04\n",
            "Epoch 30/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3157 - masked_accuracy: 0.8812   \n",
            "Epoch 30: val_masked_accuracy improved from 0.85231 to 0.85379, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 916s 335ms/step - loss: 0.3157 - masked_accuracy: 0.8812 - val_loss: 0.4069 - val_masked_accuracy: 0.8538 - lr: 1.0000e-04\n",
            "Epoch 31/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3115 - masked_accuracy: 0.8830   \n",
            "Epoch 31: val_masked_accuracy improved from 0.85379 to 0.85520, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 916s 335ms/step - loss: 0.3115 - masked_accuracy: 0.8830 - val_loss: 0.4024 - val_masked_accuracy: 0.8552 - lr: 1.0000e-04\n",
            "Epoch 32/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3068 - masked_accuracy: 0.8850   \n",
            "Epoch 32: val_masked_accuracy improved from 0.85520 to 0.85523, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 937s 343ms/step - loss: 0.3068 - masked_accuracy: 0.8850 - val_loss: 0.4118 - val_masked_accuracy: 0.8552 - lr: 1.0000e-04\n",
            "Epoch 33/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.3028 - masked_accuracy: 0.8866   \n",
            "Epoch 33: val_masked_accuracy improved from 0.85523 to 0.85687, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 916s 336ms/step - loss: 0.3028 - masked_accuracy: 0.8866 - val_loss: 0.3917 - val_masked_accuracy: 0.8569 - lr: 1.0000e-04\n",
            "Epoch 34/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2993 - masked_accuracy: 0.8881   \n",
            "Epoch 34: val_masked_accuracy improved from 0.85687 to 0.85885, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2993 - masked_accuracy: 0.8881 - val_loss: 0.3926 - val_masked_accuracy: 0.8588 - lr: 1.0000e-04\n",
            "Epoch 35/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2955 - masked_accuracy: 0.8896   \n",
            "Epoch 35: val_masked_accuracy did not improve from 0.85885\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2955 - masked_accuracy: 0.8896 - val_loss: 0.3940 - val_masked_accuracy: 0.8580 - lr: 1.0000e-04\n",
            "Epoch 36/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2913 - masked_accuracy: 0.8914   \n",
            "Epoch 36: val_masked_accuracy improved from 0.85885 to 0.85901, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.2913 - masked_accuracy: 0.8914 - val_loss: 0.3914 - val_masked_accuracy: 0.8590 - lr: 1.0000e-04\n",
            "Epoch 37/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2886 - masked_accuracy: 0.8926   \n",
            "Epoch 37: val_masked_accuracy improved from 0.85901 to 0.85972, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2886 - masked_accuracy: 0.8926 - val_loss: 0.3926 - val_masked_accuracy: 0.8597 - lr: 1.0000e-04\n",
            "Epoch 38/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2843 - masked_accuracy: 0.8943   \n",
            "Epoch 38: val_masked_accuracy improved from 0.85972 to 0.86232, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.2843 - masked_accuracy: 0.8943 - val_loss: 0.3972 - val_masked_accuracy: 0.8623 - lr: 1.0000e-04\n",
            "Epoch 39/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2816 - masked_accuracy: 0.8954   \n",
            "Epoch 39: val_masked_accuracy did not improve from 0.86232\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2816 - masked_accuracy: 0.8954 - val_loss: 0.4008 - val_masked_accuracy: 0.8604 - lr: 1.0000e-04\n",
            "Epoch 40/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2783 - masked_accuracy: 0.8967   \n",
            "Epoch 40: val_masked_accuracy improved from 0.86232 to 0.86319, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 919s 337ms/step - loss: 0.2783 - masked_accuracy: 0.8967 - val_loss: 0.4047 - val_masked_accuracy: 0.8632 - lr: 1.0000e-04\n",
            "Epoch 41/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2753 - masked_accuracy: 0.8981   \n",
            "Epoch 41: val_masked_accuracy did not improve from 0.86319\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2753 - masked_accuracy: 0.8981 - val_loss: 0.4075 - val_masked_accuracy: 0.8605 - lr: 1.0000e-04\n",
            "Epoch 42/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2730 - masked_accuracy: 0.8990   \n",
            "Epoch 42: val_masked_accuracy improved from 0.86319 to 0.86498, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 919s 337ms/step - loss: 0.2730 - masked_accuracy: 0.8990 - val_loss: 0.3957 - val_masked_accuracy: 0.8650 - lr: 1.0000e-04\n",
            "Epoch 43/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2703 - masked_accuracy: 0.9002   \n",
            "Epoch 43: val_masked_accuracy did not improve from 0.86498\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2703 - masked_accuracy: 0.9002 - val_loss: 0.3931 - val_masked_accuracy: 0.8647 - lr: 1.0000e-04\n",
            "Epoch 44/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2671 - masked_accuracy: 0.9014   \n",
            "Epoch 44: val_masked_accuracy improved from 0.86498 to 0.86582, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 917s 336ms/step - loss: 0.2671 - masked_accuracy: 0.9014 - val_loss: 0.3928 - val_masked_accuracy: 0.8658 - lr: 1.0000e-04\n",
            "Epoch 45/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2645 - masked_accuracy: 0.9025   \n",
            "Epoch 45: val_masked_accuracy did not improve from 0.86582\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.2645 - masked_accuracy: 0.9025 - val_loss: 0.3918 - val_masked_accuracy: 0.8644 - lr: 1.0000e-04\n",
            "Epoch 46/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2628 - masked_accuracy: 0.9032   \n",
            "Epoch 46: val_masked_accuracy did not improve from 0.86582\n",
            "2730/2730 [==============================] - 918s 336ms/step - loss: 0.2628 - masked_accuracy: 0.9032 - val_loss: 0.4010 - val_masked_accuracy: 0.8653 - lr: 1.0000e-04\n",
            "Epoch 47/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2598 - masked_accuracy: 0.9044   \n",
            "Epoch 47: val_masked_accuracy did not improve from 0.86582\n",
            "2730/2730 [==============================] - 919s 337ms/step - loss: 0.2598 - masked_accuracy: 0.9044 - val_loss: 0.3918 - val_masked_accuracy: 0.8655 - lr: 1.0000e-04\n",
            "Epoch 48/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2578 - masked_accuracy: 0.9053   \n",
            "Epoch 48: val_masked_accuracy improved from 0.86582 to 0.86652, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 920s 337ms/step - loss: 0.2578 - masked_accuracy: 0.9053 - val_loss: 0.3994 - val_masked_accuracy: 0.8665 - lr: 1.0000e-04\n",
            "Epoch 49/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2555 - masked_accuracy: 0.9062   \n",
            "Epoch 49: val_masked_accuracy did not improve from 0.86652\n",
            "2730/2730 [==============================] - 920s 337ms/step - loss: 0.2555 - masked_accuracy: 0.9062 - val_loss: 0.4052 - val_masked_accuracy: 0.8658 - lr: 1.0000e-04\n",
            "Epoch 50/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2529 - masked_accuracy: 0.9072   \n",
            "Epoch 50: val_masked_accuracy did not improve from 0.86652\n",
            "2730/2730 [==============================] - 920s 337ms/step - loss: 0.2529 - masked_accuracy: 0.9072 - val_loss: 0.3998 - val_masked_accuracy: 0.8662 - lr: 1.0000e-04\n",
            "Epoch 51/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2514 - masked_accuracy: 0.9079   \n",
            "Epoch 51: val_masked_accuracy improved from 0.86652 to 0.86828, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 923s 338ms/step - loss: 0.2514 - masked_accuracy: 0.9079 - val_loss: 0.4010 - val_masked_accuracy: 0.8683 - lr: 1.0000e-04\n",
            "Epoch 52/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2495 - masked_accuracy: 0.9086   \n",
            "Epoch 52: val_masked_accuracy improved from 0.86828 to 0.86886, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 927s 340ms/step - loss: 0.2495 - masked_accuracy: 0.9086 - val_loss: 0.4119 - val_masked_accuracy: 0.8689 - lr: 1.0000e-04\n",
            "Epoch 53/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2475 - masked_accuracy: 0.9095   \n",
            "Epoch 53: val_masked_accuracy improved from 0.86886 to 0.86978, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 930s 341ms/step - loss: 0.2475 - masked_accuracy: 0.9095 - val_loss: 0.3904 - val_masked_accuracy: 0.8698 - lr: 1.0000e-04\n",
            "Epoch 54/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2451 - masked_accuracy: 0.9105   \n",
            "Epoch 54: val_masked_accuracy improved from 0.86978 to 0.87009, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 936s 343ms/step - loss: 0.2451 - masked_accuracy: 0.9105 - val_loss: 0.3947 - val_masked_accuracy: 0.8701 - lr: 1.0000e-04\n",
            "Epoch 55/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2439 - masked_accuracy: 0.9110   \n",
            "Epoch 55: val_masked_accuracy did not improve from 0.87009\n",
            "2730/2730 [==============================] - 961s 352ms/step - loss: 0.2439 - masked_accuracy: 0.9110 - val_loss: 0.4071 - val_masked_accuracy: 0.8699 - lr: 1.0000e-04\n",
            "Epoch 56/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2422 - masked_accuracy: 0.9117   \n",
            "Epoch 56: val_masked_accuracy did not improve from 0.87009\n",
            "2730/2730 [==============================] - 929s 340ms/step - loss: 0.2422 - masked_accuracy: 0.9117 - val_loss: 0.3973 - val_masked_accuracy: 0.8701 - lr: 1.0000e-04\n",
            "Epoch 57/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2398 - masked_accuracy: 0.9126   \n",
            "Epoch 57: val_masked_accuracy did not improve from 0.87009\n",
            "2730/2730 [==============================] - 932s 341ms/step - loss: 0.2398 - masked_accuracy: 0.9126 - val_loss: 0.4061 - val_masked_accuracy: 0.8678 - lr: 1.0000e-04\n",
            "Epoch 58/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2385 - masked_accuracy: 0.9132   \n",
            "Epoch 58: val_masked_accuracy improved from 0.87009 to 0.87220, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 932s 342ms/step - loss: 0.2385 - masked_accuracy: 0.9132 - val_loss: 0.4016 - val_masked_accuracy: 0.8722 - lr: 1.0000e-04\n",
            "Epoch 59/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2365 - masked_accuracy: 0.9139   \n",
            "Epoch 59: val_masked_accuracy did not improve from 0.87220\n",
            "2730/2730 [==============================] - 932s 341ms/step - loss: 0.2365 - masked_accuracy: 0.9139 - val_loss: 0.4057 - val_masked_accuracy: 0.8715 - lr: 1.0000e-04\n",
            "Epoch 60/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2355 - masked_accuracy: 0.9144   \n",
            "Epoch 60: val_masked_accuracy improved from 0.87220 to 0.87333, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 936s 343ms/step - loss: 0.2355 - masked_accuracy: 0.9144 - val_loss: 0.3957 - val_masked_accuracy: 0.8733 - lr: 1.0000e-04\n",
            "Epoch 61/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2338 - masked_accuracy: 0.9151   \n",
            "Epoch 61: val_masked_accuracy did not improve from 0.87333\n",
            "2730/2730 [==============================] - 935s 342ms/step - loss: 0.2338 - masked_accuracy: 0.9151 - val_loss: 0.3967 - val_masked_accuracy: 0.8719 - lr: 1.0000e-04\n",
            "Epoch 62/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2320 - masked_accuracy: 0.9158   \n",
            "Epoch 62: val_masked_accuracy did not improve from 0.87333\n",
            "2730/2730 [==============================] - 935s 342ms/step - loss: 0.2320 - masked_accuracy: 0.9158 - val_loss: 0.3985 - val_masked_accuracy: 0.8716 - lr: 1.0000e-04\n",
            "Epoch 63/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2307 - masked_accuracy: 0.9163   \n",
            "Epoch 63: val_masked_accuracy did not improve from 0.87333\n",
            "2730/2730 [==============================] - 934s 342ms/step - loss: 0.2307 - masked_accuracy: 0.9163 - val_loss: 0.3931 - val_masked_accuracy: 0.8710 - lr: 1.0000e-04\n",
            "Epoch 64/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2293 - masked_accuracy: 0.9169   \n",
            "Epoch 64: val_masked_accuracy did not improve from 0.87333\n",
            "2730/2730 [==============================] - 933s 342ms/step - loss: 0.2293 - masked_accuracy: 0.9169 - val_loss: 0.4000 - val_masked_accuracy: 0.8721 - lr: 1.0000e-04\n",
            "Epoch 65/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2280 - masked_accuracy: 0.9174   \n",
            "Epoch 65: val_masked_accuracy improved from 0.87333 to 0.87386, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 937s 343ms/step - loss: 0.2280 - masked_accuracy: 0.9174 - val_loss: 0.3987 - val_masked_accuracy: 0.8739 - lr: 1.0000e-04\n",
            "Epoch 66/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2264 - masked_accuracy: 0.9181   \n",
            "Epoch 66: val_masked_accuracy improved from 0.87386 to 0.87466, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 936s 343ms/step - loss: 0.2264 - masked_accuracy: 0.9181 - val_loss: 0.3953 - val_masked_accuracy: 0.8747 - lr: 1.0000e-04\n",
            "Epoch 67/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2250 - masked_accuracy: 0.9186   \n",
            "Epoch 67: val_masked_accuracy did not improve from 0.87466\n",
            "2730/2730 [==============================] - 935s 342ms/step - loss: 0.2250 - masked_accuracy: 0.9186 - val_loss: 0.3958 - val_masked_accuracy: 0.8745 - lr: 1.0000e-04\n",
            "Epoch 68/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2241 - masked_accuracy: 0.9190   \n",
            "Epoch 68: val_masked_accuracy improved from 0.87466 to 0.87473, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 936s 343ms/step - loss: 0.2241 - masked_accuracy: 0.9190 - val_loss: 0.4035 - val_masked_accuracy: 0.8747 - lr: 1.0000e-04\n",
            "Epoch 69/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2224 - masked_accuracy: 0.9197   \n",
            "Epoch 69: val_masked_accuracy improved from 0.87473 to 0.87510, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 939s 344ms/step - loss: 0.2224 - masked_accuracy: 0.9197 - val_loss: 0.3942 - val_masked_accuracy: 0.8751 - lr: 1.0000e-04\n",
            "Epoch 70/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2213 - masked_accuracy: 0.9202   \n",
            "Epoch 70: val_masked_accuracy did not improve from 0.87510\n",
            "2730/2730 [==============================] - 938s 343ms/step - loss: 0.2213 - masked_accuracy: 0.9202 - val_loss: 0.4059 - val_masked_accuracy: 0.8731 - lr: 1.0000e-04\n",
            "Epoch 71/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2199 - masked_accuracy: 0.9207   \n",
            "Epoch 71: val_masked_accuracy improved from 0.87510 to 0.87564, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 941s 345ms/step - loss: 0.2199 - masked_accuracy: 0.9207 - val_loss: 0.4156 - val_masked_accuracy: 0.8756 - lr: 1.0000e-04\n",
            "Epoch 72/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2190 - masked_accuracy: 0.9210   \n",
            "Epoch 72: val_masked_accuracy did not improve from 0.87564\n",
            "2730/2730 [==============================] - 940s 344ms/step - loss: 0.2190 - masked_accuracy: 0.9210 - val_loss: 0.4164 - val_masked_accuracy: 0.8743 - lr: 1.0000e-04\n",
            "Epoch 73/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2179 - masked_accuracy: 0.9215   \n",
            "Epoch 73: val_masked_accuracy improved from 0.87564 to 0.87608, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 939s 344ms/step - loss: 0.2179 - masked_accuracy: 0.9215 - val_loss: 0.4049 - val_masked_accuracy: 0.8761 - lr: 1.0000e-04\n",
            "Epoch 74/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2164 - masked_accuracy: 0.9221   \n",
            "Epoch 74: val_masked_accuracy did not improve from 0.87608\n",
            "2730/2730 [==============================] - 940s 344ms/step - loss: 0.2164 - masked_accuracy: 0.9221 - val_loss: 0.4092 - val_masked_accuracy: 0.8757 - lr: 1.0000e-04\n",
            "Epoch 75/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2155 - masked_accuracy: 0.9225   \n",
            "Epoch 75: val_masked_accuracy improved from 0.87608 to 0.87703, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 1111s 407ms/step - loss: 0.2155 - masked_accuracy: 0.9225 - val_loss: 0.3974 - val_masked_accuracy: 0.8770 - lr: 1.0000e-04\n",
            "Epoch 76/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2143 - masked_accuracy: 0.9230   \n",
            "Epoch 76: val_masked_accuracy did not improve from 0.87703\n",
            "2730/2730 [==============================] - 944s 346ms/step - loss: 0.2143 - masked_accuracy: 0.9230 - val_loss: 0.4114 - val_masked_accuracy: 0.8755 - lr: 1.0000e-04\n",
            "Epoch 77/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2130 - masked_accuracy: 0.9234   \n",
            "Epoch 77: val_masked_accuracy did not improve from 0.87703\n",
            "2730/2730 [==============================] - 941s 345ms/step - loss: 0.2130 - masked_accuracy: 0.9234 - val_loss: 0.4051 - val_masked_accuracy: 0.8759 - lr: 1.0000e-04\n",
            "Epoch 78/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2124 - masked_accuracy: 0.9237   \n",
            "Epoch 78: val_masked_accuracy did not improve from 0.87703\n",
            "2730/2730 [==============================] - 938s 344ms/step - loss: 0.2124 - masked_accuracy: 0.9237 - val_loss: 0.4087 - val_masked_accuracy: 0.8755 - lr: 1.0000e-04\n",
            "Epoch 79/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2111 - masked_accuracy: 0.9242   \n",
            "Epoch 79: val_masked_accuracy improved from 0.87703 to 0.87804, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 943s 345ms/step - loss: 0.2111 - masked_accuracy: 0.9242 - val_loss: 0.4008 - val_masked_accuracy: 0.8780 - lr: 1.0000e-04\n",
            "Epoch 80/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2105 - masked_accuracy: 0.9245   \n",
            "Epoch 80: val_masked_accuracy did not improve from 0.87804\n",
            "2730/2730 [==============================] - 938s 344ms/step - loss: 0.2105 - masked_accuracy: 0.9245 - val_loss: 0.4051 - val_masked_accuracy: 0.8770 - lr: 1.0000e-04\n",
            "Epoch 81/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2091 - masked_accuracy: 0.9250   \n",
            "Epoch 81: val_masked_accuracy did not improve from 0.87804\n",
            "2730/2730 [==============================] - 943s 345ms/step - loss: 0.2091 - masked_accuracy: 0.9250 - val_loss: 0.3900 - val_masked_accuracy: 0.8768 - lr: 1.0000e-04\n",
            "Epoch 82/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2080 - masked_accuracy: 0.9254   \n",
            "Epoch 82: val_masked_accuracy did not improve from 0.87804\n",
            "2730/2730 [==============================] - 943s 345ms/step - loss: 0.2080 - masked_accuracy: 0.9254 - val_loss: 0.4101 - val_masked_accuracy: 0.8778 - lr: 1.0000e-04\n",
            "Epoch 83/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2075 - masked_accuracy: 0.9257   \n",
            "Epoch 83: val_masked_accuracy did not improve from 0.87804\n",
            "2730/2730 [==============================] - 944s 346ms/step - loss: 0.2075 - masked_accuracy: 0.9257 - val_loss: 0.4113 - val_masked_accuracy: 0.8758 - lr: 1.0000e-04\n",
            "Epoch 84/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2061 - masked_accuracy: 0.9263   \n",
            "Epoch 84: val_masked_accuracy did not improve from 0.87804\n",
            "2730/2730 [==============================] - 943s 345ms/step - loss: 0.2061 - masked_accuracy: 0.9263 - val_loss: 0.4078 - val_masked_accuracy: 0.8775 - lr: 1.0000e-04\n",
            "Epoch 85/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2057 - masked_accuracy: 0.9265   \n",
            "Epoch 85: val_masked_accuracy improved from 0.87804 to 0.87845, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 941s 345ms/step - loss: 0.2057 - masked_accuracy: 0.9265 - val_loss: 0.4056 - val_masked_accuracy: 0.8785 - lr: 1.0000e-04\n",
            "Epoch 86/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2046 - masked_accuracy: 0.9268   \n",
            "Epoch 86: val_masked_accuracy improved from 0.87845 to 0.87866, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 945s 346ms/step - loss: 0.2046 - masked_accuracy: 0.9268 - val_loss: 0.4205 - val_masked_accuracy: 0.8787 - lr: 1.0000e-04\n",
            "Epoch 87/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2034 - masked_accuracy: 0.9274   \n",
            "Epoch 87: val_masked_accuracy improved from 0.87866 to 0.87921, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 946s 347ms/step - loss: 0.2034 - masked_accuracy: 0.9274 - val_loss: 0.4072 - val_masked_accuracy: 0.8792 - lr: 1.0000e-04\n",
            "Epoch 88/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2028 - masked_accuracy: 0.9276   \n",
            "Epoch 88: val_masked_accuracy did not improve from 0.87921\n",
            "2730/2730 [==============================] - 944s 346ms/step - loss: 0.2028 - masked_accuracy: 0.9276 - val_loss: 0.3966 - val_masked_accuracy: 0.8778 - lr: 1.0000e-04\n",
            "Epoch 89/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2017 - masked_accuracy: 0.9279   \n",
            "Epoch 89: val_masked_accuracy did not improve from 0.87921\n",
            "2730/2730 [==============================] - 943s 345ms/step - loss: 0.2017 - masked_accuracy: 0.9279 - val_loss: 0.4090 - val_masked_accuracy: 0.8780 - lr: 1.0000e-04\n",
            "Epoch 90/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2010 - masked_accuracy: 0.9282   \n",
            "Epoch 90: val_masked_accuracy did not improve from 0.87921\n",
            "2730/2730 [==============================] - 945s 346ms/step - loss: 0.2010 - masked_accuracy: 0.9282 - val_loss: 0.3985 - val_masked_accuracy: 0.8787 - lr: 1.0000e-04\n",
            "Epoch 91/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.2005 - masked_accuracy: 0.9285   \n",
            "Epoch 91: val_masked_accuracy did not improve from 0.87921\n",
            "2730/2730 [==============================] - 942s 345ms/step - loss: 0.2005 - masked_accuracy: 0.9285 - val_loss: 0.4133 - val_masked_accuracy: 0.8791 - lr: 1.0000e-04\n",
            "Epoch 92/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1992 - masked_accuracy: 0.9290   \n",
            "Epoch 92: val_masked_accuracy improved from 0.87921 to 0.87952, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 942s 345ms/step - loss: 0.1992 - masked_accuracy: 0.9290 - val_loss: 0.4093 - val_masked_accuracy: 0.8795 - lr: 1.0000e-04\n",
            "Epoch 93/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1985 - masked_accuracy: 0.9292   \n",
            "Epoch 93: val_masked_accuracy did not improve from 0.87952\n",
            "2730/2730 [==============================] - 940s 344ms/step - loss: 0.1985 - masked_accuracy: 0.9292 - val_loss: 0.4052 - val_masked_accuracy: 0.8786 - lr: 1.0000e-04\n",
            "Epoch 94/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1974 - masked_accuracy: 0.9297   \n",
            "Epoch 94: val_masked_accuracy did not improve from 0.87952\n",
            "2730/2730 [==============================] - 944s 346ms/step - loss: 0.1974 - masked_accuracy: 0.9297 - val_loss: 0.4131 - val_masked_accuracy: 0.8791 - lr: 1.0000e-04\n",
            "Epoch 95/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1966 - masked_accuracy: 0.9300   \n",
            "Epoch 95: val_masked_accuracy did not improve from 0.87952\n",
            "2730/2730 [==============================] - 943s 345ms/step - loss: 0.1966 - masked_accuracy: 0.9300 - val_loss: 0.4082 - val_masked_accuracy: 0.8793 - lr: 1.0000e-04\n",
            "Epoch 96/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1966 - masked_accuracy: 0.9300   \n",
            "Epoch 96: val_masked_accuracy did not improve from 0.87952\n",
            "2730/2730 [==============================] - 946s 347ms/step - loss: 0.1966 - masked_accuracy: 0.9300 - val_loss: 0.4141 - val_masked_accuracy: 0.8794 - lr: 1.0000e-04\n",
            "Epoch 97/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1955 - masked_accuracy: 0.9305   \n",
            "Epoch 97: val_masked_accuracy improved from 0.87952 to 0.88051, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 946s 347ms/step - loss: 0.1955 - masked_accuracy: 0.9305 - val_loss: 0.4036 - val_masked_accuracy: 0.8805 - lr: 1.0000e-04\n",
            "Epoch 98/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1950 - masked_accuracy: 0.9306   \n",
            "Epoch 98: val_masked_accuracy did not improve from 0.88051\n",
            "2730/2730 [==============================] - 945s 346ms/step - loss: 0.1950 - masked_accuracy: 0.9306 - val_loss: 0.4189 - val_masked_accuracy: 0.8785 - lr: 1.0000e-04\n",
            "Epoch 99/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1939 - masked_accuracy: 0.9310   \n",
            "Epoch 99: val_masked_accuracy did not improve from 0.88051\n",
            "2730/2730 [==============================] - 943s 346ms/step - loss: 0.1939 - masked_accuracy: 0.9310 - val_loss: 0.4042 - val_masked_accuracy: 0.8793 - lr: 1.0000e-04\n",
            "Epoch 100/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1935 - masked_accuracy: 0.9313   \n",
            "Epoch 100: val_masked_accuracy did not improve from 0.88051\n",
            "2730/2730 [==============================] - 1010s 370ms/step - loss: 0.1935 - masked_accuracy: 0.9313 - val_loss: 0.4236 - val_masked_accuracy: 0.8789 - lr: 1.0000e-04\n",
            "Epoch 101/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1923 - masked_accuracy: 0.9317   \n",
            "Epoch 101: val_masked_accuracy improved from 0.88051 to 0.88070, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 944s 346ms/step - loss: 0.1923 - masked_accuracy: 0.9317 - val_loss: 0.4075 - val_masked_accuracy: 0.8807 - lr: 1.0000e-04\n",
            "Epoch 102/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1922 - masked_accuracy: 0.9318   \n",
            "Epoch 102: val_masked_accuracy did not improve from 0.88070\n",
            "2730/2730 [==============================] - 945s 346ms/step - loss: 0.1922 - masked_accuracy: 0.9318 - val_loss: 0.4104 - val_masked_accuracy: 0.8806 - lr: 1.0000e-04\n",
            "Epoch 103/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1910 - masked_accuracy: 0.9323   \n",
            "Epoch 103: val_masked_accuracy improved from 0.88070 to 0.88097, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 945s 346ms/step - loss: 0.1910 - masked_accuracy: 0.9323 - val_loss: 0.4104 - val_masked_accuracy: 0.8810 - lr: 1.0000e-04\n",
            "Epoch 104/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1907 - masked_accuracy: 0.9325   \n",
            "Epoch 104: val_masked_accuracy did not improve from 0.88097\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1907 - masked_accuracy: 0.9325 - val_loss: 0.4233 - val_masked_accuracy: 0.8809 - lr: 1.0000e-04\n",
            "Epoch 105/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1899 - masked_accuracy: 0.9326   \n",
            "Epoch 105: val_masked_accuracy did not improve from 0.88097\n",
            "2730/2730 [==============================] - 951s 348ms/step - loss: 0.1899 - masked_accuracy: 0.9326 - val_loss: 0.4355 - val_masked_accuracy: 0.8798 - lr: 1.0000e-04\n",
            "Epoch 106/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1891 - masked_accuracy: 0.9329   \n",
            "Epoch 106: val_masked_accuracy improved from 0.88097 to 0.88114, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 953s 349ms/step - loss: 0.1891 - masked_accuracy: 0.9329 - val_loss: 0.4188 - val_masked_accuracy: 0.8811 - lr: 1.0000e-04\n",
            "Epoch 107/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1886 - masked_accuracy: 0.9332   \n",
            "Epoch 107: val_masked_accuracy improved from 0.88114 to 0.88137, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 952s 349ms/step - loss: 0.1886 - masked_accuracy: 0.9332 - val_loss: 0.4161 - val_masked_accuracy: 0.8814 - lr: 1.0000e-04\n",
            "Epoch 108/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1874 - masked_accuracy: 0.9336   \n",
            "Epoch 108: val_masked_accuracy did not improve from 0.88137\n",
            "2730/2730 [==============================] - 953s 349ms/step - loss: 0.1874 - masked_accuracy: 0.9336 - val_loss: 0.4118 - val_masked_accuracy: 0.8803 - lr: 1.0000e-04\n",
            "Epoch 109/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1876 - masked_accuracy: 0.9336   \n",
            "Epoch 109: val_masked_accuracy improved from 0.88137 to 0.88186, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1876 - masked_accuracy: 0.9336 - val_loss: 0.4205 - val_masked_accuracy: 0.8819 - lr: 1.0000e-04\n",
            "Epoch 110/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1866 - masked_accuracy: 0.9340   \n",
            "Epoch 110: val_masked_accuracy did not improve from 0.88186\n",
            "2730/2730 [==============================] - 948s 347ms/step - loss: 0.1866 - masked_accuracy: 0.9340 - val_loss: 0.4246 - val_masked_accuracy: 0.8810 - lr: 1.0000e-04\n",
            "Epoch 111/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1861 - masked_accuracy: 0.9342   \n",
            "Epoch 111: val_masked_accuracy did not improve from 0.88186\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1861 - masked_accuracy: 0.9342 - val_loss: 0.4302 - val_masked_accuracy: 0.8808 - lr: 1.0000e-04\n",
            "Epoch 112/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1852 - masked_accuracy: 0.9345   \n",
            "Epoch 112: val_masked_accuracy improved from 0.88186 to 0.88201, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 950s 348ms/step - loss: 0.1852 - masked_accuracy: 0.9345 - val_loss: 0.4245 - val_masked_accuracy: 0.8820 - lr: 1.0000e-04\n",
            "Epoch 113/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1848 - masked_accuracy: 0.9347   \n",
            "Epoch 113: val_masked_accuracy did not improve from 0.88201\n",
            "2730/2730 [==============================] - 950s 348ms/step - loss: 0.1848 - masked_accuracy: 0.9347 - val_loss: 0.4227 - val_masked_accuracy: 0.8820 - lr: 1.0000e-04\n",
            "Epoch 114/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1847 - masked_accuracy: 0.9348   \n",
            "Epoch 114: val_masked_accuracy did not improve from 0.88201\n",
            "2730/2730 [==============================] - 951s 348ms/step - loss: 0.1847 - masked_accuracy: 0.9348 - val_loss: 0.4247 - val_masked_accuracy: 0.8814 - lr: 1.0000e-04\n",
            "Epoch 115/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1839 - masked_accuracy: 0.9351   \n",
            "Epoch 115: val_masked_accuracy improved from 0.88201 to 0.88251, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1839 - masked_accuracy: 0.9351 - val_loss: 0.4287 - val_masked_accuracy: 0.8825 - lr: 1.0000e-04\n",
            "Epoch 116/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1828 - masked_accuracy: 0.9355   \n",
            "Epoch 116: val_masked_accuracy did not improve from 0.88251\n",
            "2730/2730 [==============================] - 947s 347ms/step - loss: 0.1828 - masked_accuracy: 0.9355 - val_loss: 0.4309 - val_masked_accuracy: 0.8822 - lr: 1.0000e-04\n",
            "Epoch 117/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1827 - masked_accuracy: 0.9356   \n",
            "Epoch 117: val_masked_accuracy did not improve from 0.88251\n",
            "2730/2730 [==============================] - 948s 347ms/step - loss: 0.1827 - masked_accuracy: 0.9356 - val_loss: 0.4345 - val_masked_accuracy: 0.8816 - lr: 1.0000e-04\n",
            "Epoch 118/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1820 - masked_accuracy: 0.9359   \n",
            "Epoch 118: val_masked_accuracy did not improve from 0.88251\n",
            "2730/2730 [==============================] - 948s 347ms/step - loss: 0.1820 - masked_accuracy: 0.9359 - val_loss: 0.4331 - val_masked_accuracy: 0.8823 - lr: 1.0000e-04\n",
            "Epoch 119/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1815 - masked_accuracy: 0.9360   \n",
            "Epoch 119: val_masked_accuracy did not improve from 0.88251\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1815 - masked_accuracy: 0.9360 - val_loss: 0.4169 - val_masked_accuracy: 0.8824 - lr: 1.0000e-04\n",
            "Epoch 120/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1806 - masked_accuracy: 0.9364   \n",
            "Epoch 120: val_masked_accuracy did not improve from 0.88251\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1806 - masked_accuracy: 0.9364 - val_loss: 0.4321 - val_masked_accuracy: 0.8818 - lr: 1.0000e-04\n",
            "Epoch 121/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1803 - masked_accuracy: 0.9365   \n",
            "Epoch 121: val_masked_accuracy improved from 0.88251 to 0.88258, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 950s 348ms/step - loss: 0.1803 - masked_accuracy: 0.9365 - val_loss: 0.4237 - val_masked_accuracy: 0.8826 - lr: 1.0000e-04\n",
            "Epoch 122/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1795 - masked_accuracy: 0.9367   \n",
            "Epoch 122: val_masked_accuracy improved from 0.88258 to 0.88303, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 951s 348ms/step - loss: 0.1795 - masked_accuracy: 0.9367 - val_loss: 0.4266 - val_masked_accuracy: 0.8830 - lr: 1.0000e-04\n",
            "Epoch 123/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1791 - masked_accuracy: 0.9370   \n",
            "Epoch 123: val_masked_accuracy did not improve from 0.88303\n",
            "2730/2730 [==============================] - 946s 346ms/step - loss: 0.1791 - masked_accuracy: 0.9370 - val_loss: 0.4336 - val_masked_accuracy: 0.8825 - lr: 1.0000e-04\n",
            "Epoch 124/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1789 - masked_accuracy: 0.9370   \n",
            "Epoch 124: val_masked_accuracy did not improve from 0.88303\n",
            "2730/2730 [==============================] - 963s 353ms/step - loss: 0.1789 - masked_accuracy: 0.9370 - val_loss: 0.4346 - val_masked_accuracy: 0.8816 - lr: 1.0000e-04\n",
            "Epoch 125/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1787 - masked_accuracy: 0.9371   \n",
            "Epoch 125: val_masked_accuracy did not improve from 0.88303\n",
            "2730/2730 [==============================] - 948s 347ms/step - loss: 0.1787 - masked_accuracy: 0.9371 - val_loss: 0.4212 - val_masked_accuracy: 0.8828 - lr: 1.0000e-04\n",
            "Epoch 126/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1778 - masked_accuracy: 0.9374   \n",
            "Epoch 126: val_masked_accuracy did not improve from 0.88303\n",
            "2730/2730 [==============================] - 951s 348ms/step - loss: 0.1778 - masked_accuracy: 0.9374 - val_loss: 0.4206 - val_masked_accuracy: 0.8828 - lr: 1.0000e-04\n",
            "Epoch 127/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1768 - masked_accuracy: 0.9378   \n",
            "Epoch 127: val_masked_accuracy improved from 0.88303 to 0.88339, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 950s 348ms/step - loss: 0.1768 - masked_accuracy: 0.9378 - val_loss: 0.4253 - val_masked_accuracy: 0.8834 - lr: 1.0000e-04\n",
            "Epoch 128/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1767 - masked_accuracy: 0.9379   \n",
            "Epoch 128: val_masked_accuracy did not improve from 0.88339\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1767 - masked_accuracy: 0.9379 - val_loss: 0.4392 - val_masked_accuracy: 0.8829 - lr: 1.0000e-04\n",
            "Epoch 129/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1762 - masked_accuracy: 0.9381   \n",
            "Epoch 129: val_masked_accuracy improved from 0.88339 to 0.88378, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 949s 348ms/step - loss: 0.1762 - masked_accuracy: 0.9381 - val_loss: 0.4388 - val_masked_accuracy: 0.8838 - lr: 1.0000e-04\n",
            "Epoch 130/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1758 - masked_accuracy: 0.9382   \n",
            "Epoch 130: val_masked_accuracy did not improve from 0.88378\n",
            "2730/2730 [==============================] - 950s 348ms/step - loss: 0.1758 - masked_accuracy: 0.9382 - val_loss: 0.4294 - val_masked_accuracy: 0.8833 - lr: 1.0000e-04\n",
            "Epoch 131/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1757 - masked_accuracy: 0.9383   \n",
            "Epoch 131: val_masked_accuracy did not improve from 0.88378\n",
            "2730/2730 [==============================] - 958s 351ms/step - loss: 0.1757 - masked_accuracy: 0.9383 - val_loss: 0.4326 - val_masked_accuracy: 0.8836 - lr: 1.0000e-04\n",
            "Epoch 132/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1745 - masked_accuracy: 0.9387   \n",
            "Epoch 132: val_masked_accuracy did not improve from 0.88378\n",
            "2730/2730 [==============================] - 950s 348ms/step - loss: 0.1745 - masked_accuracy: 0.9387 - val_loss: 0.4266 - val_masked_accuracy: 0.8828 - lr: 1.0000e-04\n",
            "Epoch 133/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1743 - masked_accuracy: 0.9388   \n",
            "Epoch 133: val_masked_accuracy did not improve from 0.88378\n",
            "2730/2730 [==============================] - 952s 349ms/step - loss: 0.1743 - masked_accuracy: 0.9388 - val_loss: 0.4244 - val_masked_accuracy: 0.8832 - lr: 1.0000e-04\n",
            "Epoch 134/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1741 - masked_accuracy: 0.9389   \n",
            "Epoch 134: val_masked_accuracy did not improve from 0.88378\n",
            "2730/2730 [==============================] - 951s 348ms/step - loss: 0.1741 - masked_accuracy: 0.9389 - val_loss: 0.4439 - val_masked_accuracy: 0.8837 - lr: 1.0000e-04\n",
            "Epoch 135/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1732 - masked_accuracy: 0.9392   \n",
            "Epoch 135: val_masked_accuracy did not improve from 0.88378\n",
            "2730/2730 [==============================] - 953s 349ms/step - loss: 0.1732 - masked_accuracy: 0.9392 - val_loss: 0.4328 - val_masked_accuracy: 0.8833 - lr: 1.0000e-04\n",
            "Epoch 136/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1730 - masked_accuracy: 0.9394   \n",
            "Epoch 136: val_masked_accuracy improved from 0.88378 to 0.88398, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 952s 349ms/step - loss: 0.1730 - masked_accuracy: 0.9394 - val_loss: 0.4297 - val_masked_accuracy: 0.8840 - lr: 1.0000e-04\n",
            "Epoch 137/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1728 - masked_accuracy: 0.9394   \n",
            "Epoch 137: val_masked_accuracy did not improve from 0.88398\n",
            "2730/2730 [==============================] - 953s 349ms/step - loss: 0.1728 - masked_accuracy: 0.9394 - val_loss: 0.4378 - val_masked_accuracy: 0.8839 - lr: 1.0000e-04\n",
            "Epoch 138/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1719 - masked_accuracy: 0.9397   \n",
            "Epoch 138: val_masked_accuracy did not improve from 0.88398\n",
            "2730/2730 [==============================] - 954s 349ms/step - loss: 0.1719 - masked_accuracy: 0.9397 - val_loss: 0.4253 - val_masked_accuracy: 0.8836 - lr: 1.0000e-04\n",
            "Epoch 139/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1719 - masked_accuracy: 0.9398   \n",
            "Epoch 139: val_masked_accuracy did not improve from 0.88398\n",
            "2730/2730 [==============================] - 952s 349ms/step - loss: 0.1719 - masked_accuracy: 0.9398 - val_loss: 0.4373 - val_masked_accuracy: 0.8820 - lr: 1.0000e-04\n",
            "Epoch 140/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1713 - masked_accuracy: 0.9400   \n",
            "Epoch 140: val_masked_accuracy improved from 0.88398 to 0.88458, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 954s 350ms/step - loss: 0.1713 - masked_accuracy: 0.9400 - val_loss: 0.4363 - val_masked_accuracy: 0.8846 - lr: 1.0000e-04\n",
            "Epoch 141/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1706 - masked_accuracy: 0.9402   \n",
            "Epoch 141: val_masked_accuracy did not improve from 0.88458\n",
            "2730/2730 [==============================] - 952s 349ms/step - loss: 0.1706 - masked_accuracy: 0.9402 - val_loss: 0.4269 - val_masked_accuracy: 0.8835 - lr: 1.0000e-04\n",
            "Epoch 142/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1704 - masked_accuracy: 0.9404   \n",
            "Epoch 142: val_masked_accuracy did not improve from 0.88458\n",
            "2730/2730 [==============================] - 957s 351ms/step - loss: 0.1704 - masked_accuracy: 0.9404 - val_loss: 0.4408 - val_masked_accuracy: 0.8841 - lr: 1.0000e-04\n",
            "Epoch 143/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1700 - masked_accuracy: 0.9405   \n",
            "Epoch 143: val_masked_accuracy improved from 0.88458 to 0.88463, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 956s 350ms/step - loss: 0.1700 - masked_accuracy: 0.9405 - val_loss: 0.4460 - val_masked_accuracy: 0.8846 - lr: 1.0000e-04\n",
            "Epoch 144/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1695 - masked_accuracy: 0.9407   \n",
            "Epoch 144: val_masked_accuracy did not improve from 0.88463\n",
            "2730/2730 [==============================] - 956s 350ms/step - loss: 0.1695 - masked_accuracy: 0.9407 - val_loss: 0.4406 - val_masked_accuracy: 0.8840 - lr: 1.0000e-04\n",
            "Epoch 145/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1688 - masked_accuracy: 0.9410   \n",
            "Epoch 145: val_masked_accuracy did not improve from 0.88463\n",
            "2730/2730 [==============================] - 953s 349ms/step - loss: 0.1688 - masked_accuracy: 0.9410 - val_loss: 0.4534 - val_masked_accuracy: 0.8846 - lr: 1.0000e-04\n",
            "Epoch 146/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1690 - masked_accuracy: 0.9409   \n",
            "Epoch 146: val_masked_accuracy did not improve from 0.88463\n",
            "2730/2730 [==============================] - 955s 350ms/step - loss: 0.1690 - masked_accuracy: 0.9409 - val_loss: 0.4499 - val_masked_accuracy: 0.8834 - lr: 1.0000e-04\n",
            "Epoch 147/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1681 - masked_accuracy: 0.9412   \n",
            "Epoch 147: val_masked_accuracy did not improve from 0.88463\n",
            "2730/2730 [==============================] - 955s 350ms/step - loss: 0.1681 - masked_accuracy: 0.9412 - val_loss: 0.4477 - val_masked_accuracy: 0.8832 - lr: 1.0000e-04\n",
            "Epoch 148/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1681 - masked_accuracy: 0.9413   \n",
            "Epoch 148: val_masked_accuracy improved from 0.88463 to 0.88526, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 957s 351ms/step - loss: 0.1681 - masked_accuracy: 0.9413 - val_loss: 0.4440 - val_masked_accuracy: 0.8853 - lr: 1.0000e-04\n",
            "Epoch 149/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1675 - masked_accuracy: 0.9414   \n",
            "Epoch 149: val_masked_accuracy did not improve from 0.88526\n",
            "2730/2730 [==============================] - 987s 362ms/step - loss: 0.1675 - masked_accuracy: 0.9414 - val_loss: 0.4328 - val_masked_accuracy: 0.8844 - lr: 1.0000e-04\n",
            "Epoch 150/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1671 - masked_accuracy: 0.9416   \n",
            "Epoch 150: val_masked_accuracy did not improve from 0.88526\n",
            "2730/2730 [==============================] - 958s 351ms/step - loss: 0.1671 - masked_accuracy: 0.9416 - val_loss: 0.4373 - val_masked_accuracy: 0.8829 - lr: 1.0000e-04\n",
            "Epoch 151/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1669 - masked_accuracy: 0.9418   \n",
            "Epoch 151: val_masked_accuracy improved from 0.88526 to 0.88560, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 966s 354ms/step - loss: 0.1669 - masked_accuracy: 0.9418 - val_loss: 0.4298 - val_masked_accuracy: 0.8856 - lr: 1.0000e-04\n",
            "Epoch 152/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1666 - masked_accuracy: 0.9418   \n",
            "Epoch 152: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 970s 355ms/step - loss: 0.1666 - masked_accuracy: 0.9418 - val_loss: 0.4455 - val_masked_accuracy: 0.8842 - lr: 1.0000e-04\n",
            "Epoch 153/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1662 - masked_accuracy: 0.9420   \n",
            "Epoch 153: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 957s 350ms/step - loss: 0.1662 - masked_accuracy: 0.9420 - val_loss: 0.4496 - val_masked_accuracy: 0.8844 - lr: 1.0000e-04\n",
            "Epoch 154/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1653 - masked_accuracy: 0.9424   \n",
            "Epoch 154: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 959s 351ms/step - loss: 0.1653 - masked_accuracy: 0.9424 - val_loss: 0.4391 - val_masked_accuracy: 0.8846 - lr: 1.0000e-04\n",
            "Epoch 155/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1647 - masked_accuracy: 0.9426   \n",
            "Epoch 155: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 959s 351ms/step - loss: 0.1647 - masked_accuracy: 0.9426 - val_loss: 0.4542 - val_masked_accuracy: 0.8847 - lr: 1.0000e-04\n",
            "Epoch 156/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1653 - masked_accuracy: 0.9424   \n",
            "Epoch 156: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 960s 352ms/step - loss: 0.1653 - masked_accuracy: 0.9424 - val_loss: 0.4525 - val_masked_accuracy: 0.8855 - lr: 1.0000e-04\n",
            "Epoch 157/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1646 - masked_accuracy: 0.9426   \n",
            "Epoch 157: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 959s 351ms/step - loss: 0.1646 - masked_accuracy: 0.9426 - val_loss: 0.4455 - val_masked_accuracy: 0.8841 - lr: 1.0000e-04\n",
            "Epoch 158/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1641 - masked_accuracy: 0.9428   \n",
            "Epoch 158: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 960s 352ms/step - loss: 0.1641 - masked_accuracy: 0.9428 - val_loss: 0.4511 - val_masked_accuracy: 0.8838 - lr: 1.0000e-04\n",
            "Epoch 159/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1637 - masked_accuracy: 0.9429   \n",
            "Epoch 159: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 964s 353ms/step - loss: 0.1637 - masked_accuracy: 0.9429 - val_loss: 0.4536 - val_masked_accuracy: 0.8851 - lr: 1.0000e-04\n",
            "Epoch 160/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1640 - masked_accuracy: 0.9429   \n",
            "Epoch 160: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 969s 355ms/step - loss: 0.1640 - masked_accuracy: 0.9429 - val_loss: 0.4474 - val_masked_accuracy: 0.8849 - lr: 1.0000e-04\n",
            "Epoch 161/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1626 - masked_accuracy: 0.9433   \n",
            "Epoch 161: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 976s 358ms/step - loss: 0.1626 - masked_accuracy: 0.9433 - val_loss: 0.4461 - val_masked_accuracy: 0.8843 - lr: 1.0000e-04\n",
            "Epoch 162/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1622 - masked_accuracy: 0.9436   \n",
            "Epoch 162: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 977s 358ms/step - loss: 0.1622 - masked_accuracy: 0.9436 - val_loss: 0.4505 - val_masked_accuracy: 0.8845 - lr: 1.0000e-04\n",
            "Epoch 163/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1625 - masked_accuracy: 0.9434   \n",
            "Epoch 163: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 958s 351ms/step - loss: 0.1625 - masked_accuracy: 0.9434 - val_loss: 0.4521 - val_masked_accuracy: 0.8844 - lr: 1.0000e-04\n",
            "Epoch 164/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1622 - masked_accuracy: 0.9436   \n",
            "Epoch 164: val_masked_accuracy did not improve from 0.88560\n",
            "2730/2730 [==============================] - 960s 352ms/step - loss: 0.1622 - masked_accuracy: 0.9436 - val_loss: 0.4555 - val_masked_accuracy: 0.8845 - lr: 1.0000e-04\n",
            "Epoch 165/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1419 - masked_accuracy: 0.9498   \n",
            "Epoch 165: val_masked_accuracy improved from 0.88560 to 0.88885, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 959s 351ms/step - loss: 0.1419 - masked_accuracy: 0.9498 - val_loss: 0.4631 - val_masked_accuracy: 0.8888 - lr: 5.0000e-05\n",
            "Epoch 166/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1373 - masked_accuracy: 0.9513   \n",
            "Epoch 166: val_masked_accuracy improved from 0.88885 to 0.88925, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 962s 352ms/step - loss: 0.1373 - masked_accuracy: 0.9513 - val_loss: 0.4777 - val_masked_accuracy: 0.8893 - lr: 5.0000e-05\n",
            "Epoch 167/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1359 - masked_accuracy: 0.9517   \n",
            "Epoch 167: val_masked_accuracy improved from 0.88925 to 0.88942, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 963s 353ms/step - loss: 0.1359 - masked_accuracy: 0.9517 - val_loss: 0.4717 - val_masked_accuracy: 0.8894 - lr: 5.0000e-05\n",
            "Epoch 168/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1348 - masked_accuracy: 0.9521   \n",
            "Epoch 168: val_masked_accuracy did not improve from 0.88942\n",
            "2730/2730 [==============================] - 960s 352ms/step - loss: 0.1348 - masked_accuracy: 0.9521 - val_loss: 0.4755 - val_masked_accuracy: 0.8890 - lr: 5.0000e-05\n",
            "Epoch 169/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1339 - masked_accuracy: 0.9524   \n",
            "Epoch 169: val_masked_accuracy improved from 0.88942 to 0.88956, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 963s 353ms/step - loss: 0.1339 - masked_accuracy: 0.9524 - val_loss: 0.4859 - val_masked_accuracy: 0.8896 - lr: 5.0000e-05\n",
            "Epoch 170/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1332 - masked_accuracy: 0.9525   \n",
            "Epoch 170: val_masked_accuracy did not improve from 0.88956\n",
            "2730/2730 [==============================] - 962s 352ms/step - loss: 0.1332 - masked_accuracy: 0.9525 - val_loss: 0.4799 - val_masked_accuracy: 0.8893 - lr: 5.0000e-05\n",
            "Epoch 171/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1326 - masked_accuracy: 0.9528   \n",
            "Epoch 171: val_masked_accuracy did not improve from 0.88956\n",
            "2730/2730 [==============================] - 963s 353ms/step - loss: 0.1326 - masked_accuracy: 0.9528 - val_loss: 0.4878 - val_masked_accuracy: 0.8895 - lr: 5.0000e-05\n",
            "Epoch 172/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1320 - masked_accuracy: 0.9530   \n",
            "Epoch 172: val_masked_accuracy did not improve from 0.88956\n",
            "2730/2730 [==============================] - 960s 352ms/step - loss: 0.1320 - masked_accuracy: 0.9530 - val_loss: 0.4994 - val_masked_accuracy: 0.8888 - lr: 5.0000e-05\n",
            "Epoch 173/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1315 - masked_accuracy: 0.9532   \n",
            "Epoch 173: val_masked_accuracy improved from 0.88956 to 0.88976, saving model to best_model_kernel_5.h5\n",
            "2730/2730 [==============================] - 963s 353ms/step - loss: 0.1315 - masked_accuracy: 0.9532 - val_loss: 0.4812 - val_masked_accuracy: 0.8898 - lr: 5.0000e-05\n",
            "Epoch 174/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1310 - masked_accuracy: 0.9534   \n",
            "Epoch 174: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 962s 352ms/step - loss: 0.1310 - masked_accuracy: 0.9534 - val_loss: 0.4895 - val_masked_accuracy: 0.8891 - lr: 5.0000e-05\n",
            "Epoch 175/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1307 - masked_accuracy: 0.9535   \n",
            "Epoch 175: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 959s 351ms/step - loss: 0.1307 - masked_accuracy: 0.9535 - val_loss: 0.4939 - val_masked_accuracy: 0.8885 - lr: 5.0000e-05\n",
            "Epoch 176/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1304 - masked_accuracy: 0.9536   \n",
            "Epoch 176: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 970s 355ms/step - loss: 0.1304 - masked_accuracy: 0.9536 - val_loss: 0.4873 - val_masked_accuracy: 0.8887 - lr: 5.0000e-05\n",
            "Epoch 177/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1299 - masked_accuracy: 0.9538   \n",
            "Epoch 177: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 958s 351ms/step - loss: 0.1299 - masked_accuracy: 0.9538 - val_loss: 0.4996 - val_masked_accuracy: 0.8891 - lr: 5.0000e-05\n",
            "Epoch 178/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1296 - masked_accuracy: 0.9539   \n",
            "Epoch 178: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 966s 354ms/step - loss: 0.1296 - masked_accuracy: 0.9539 - val_loss: 0.4992 - val_masked_accuracy: 0.8889 - lr: 5.0000e-05\n",
            "Epoch 179/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1293 - masked_accuracy: 0.9540   \n",
            "Epoch 179: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 967s 354ms/step - loss: 0.1293 - masked_accuracy: 0.9540 - val_loss: 0.4979 - val_masked_accuracy: 0.8892 - lr: 5.0000e-05\n",
            "Epoch 180/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1287 - masked_accuracy: 0.9542   \n",
            "Epoch 180: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 965s 353ms/step - loss: 0.1287 - masked_accuracy: 0.9542 - val_loss: 0.5002 - val_masked_accuracy: 0.8891 - lr: 5.0000e-05\n",
            "Epoch 181/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1285 - masked_accuracy: 0.9543   \n",
            "Epoch 181: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 967s 354ms/step - loss: 0.1285 - masked_accuracy: 0.9543 - val_loss: 0.5108 - val_masked_accuracy: 0.8888 - lr: 5.0000e-05\n",
            "Epoch 182/1000\n",
            "2730/2730 [==============================] - ETA: 0s - loss: 0.1280 - masked_accuracy: 0.9544   \n",
            "Epoch 182: val_masked_accuracy did not improve from 0.88976\n",
            "2730/2730 [==============================] - 962s 353ms/step - loss: 0.1280 - masked_accuracy: 0.9544 - val_loss: 0.4978 - val_masked_accuracy: 0.8892 - lr: 5.0000e-05\n",
            "Epoch 183/1000\n",
            "1844/2730 [===================>..........] - ETA: 7:13 - loss: 0.1272 - masked_accuracy: 0.9548 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function UniquePtr.__del__ at 0x00000268F3138310>\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\91810\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 74, in __del__\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 8: Model Training\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     15\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     checkpoint_callback,\n\u001b[0;32m     17\u001b[0m     LearningRateTracker(),\n\u001b[0;32m     18\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     19\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m     20\u001b[0m ]\n\u001b[1;32m---> 22\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Step 9: Plot Training Metrics\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Step 7: Compile the Model\n",
        "# ---------------------------\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),\n",
        "    loss=masked_categorical_crossentropy,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ---------------------------\n",
        "# Step 8: Model Training\n",
        "# ---------------------------\n",
        "callbacks = [\n",
        "    checkpoint_callback,\n",
        "    LearningRateTracker(),\n",
        "    EarlyStopping(monitor='loss', patience=6, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=1000,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 9: Plot Training Metrics\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['lr'])\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['masked_accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_masked_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Step 10: Evaluation Function\n",
        "# ---------------------------\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Convert one-hot encoded labels to class indices\n",
        "    y_true_classes = np.argmax(y_test, axis=-1).flatten()\n",
        "    y_pred_classes = np.argmax(y_pred, axis=-1).flatten()\n",
        "\n",
        "    # Create mask to ignore padding (class 0)\n",
        "    mask = y_true_classes != 0\n",
        "\n",
        "    # Apply mask to both true and predicted labels\n",
        "    y_true_masked = y_true_classes[mask]\n",
        "    y_pred_masked = y_pred_classes[mask]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true_masked, y_pred_masked)\n",
        "    print(f\"\\nTest Accuracy (excluding padding): {accuracy:.4f}\")\n",
        "\n",
        "    # Classification report (C=1, H=2, E=3 in our encoding)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true_masked,\n",
        "        y_pred_masked,\n",
        "        target_names=['C', 'H', 'E'],\n",
        "        labels=[1, 2, 3],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true_masked, y_pred_masked, labels=[1, 2, 3])\n",
        "\n",
        "    # Plot unnormalized confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['C', 'H', 'E'],\n",
        "                yticklabels=['C', 'H', 'E'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix (excluding padding)')\n",
        "    plt.show()\n",
        "\n",
        "    # Compute normalized confusion matrix\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Plot normalized confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=['C', 'H', 'E'],\n",
        "                yticklabels=['C', 'H', 'E'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Normalized Confusion Matrix (excluding padding)')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 11: Evaluate the Model\n",
        "# ---------------------------\n",
        "evaluate_model(model, X_test, y_test)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 12: Prediction Function\n",
        "# ---------------------------\n",
        "def predict_sequence(model, sequence):\n",
        "    original_length = len(sequence)\n",
        "    padded_sequence = sequence.ljust(max_seq_len, '0')[:max_seq_len]\n",
        "    encoded_sequence = [aa_dict.get(c, 0) for c in padded_sequence]\n",
        "    prediction = model.predict(np.array([encoded_sequence]))\n",
        "    return np.argmax(prediction[0][:original_length], axis=-1)\n",
        "\n",
        "# Example:\n",
        "# predicted_classes = predict_sequence(model, 'ACDEFG')\n",
        "# print(\"Predicted Secondary Structures:\", predicted_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9cf771-6a2a-4874-90c4-562b5baa05ce",
      "metadata": {
        "id": "5d9cf771-6a2a-4874-90c4-562b5baa05ce",
        "outputId": "d66bb69a-cfc3-4d32-b465-34b7e8ac7a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "773/773 [==============================] - 134s 170ms/step\n",
            "\n",
            "Test Accuracy (excluding padding): 0.8899\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.8727    0.8772    0.8749   3873895\n",
            "           H     0.9054    0.9179    0.9116   3479109\n",
            "           E     0.8958    0.8681    0.8817   2200142\n",
            "\n",
            "    accuracy                         0.8899   9553146\n",
            "   macro avg     0.8913    0.8877    0.8894   9553146\n",
            "weighted avg     0.8899    0.8899    0.8899   9553146\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the best saved model with custom metrics and loss function\n",
        "model = tf.keras.models.load_model(\n",
        "    \"best_model_kernel_5.h5\",\n",
        "    custom_objects={\n",
        "        'masked_accuracy': masked_accuracy,\n",
        "        'masked_categorical_crossentropy': masked_categorical_crossentropy\n",
        "    }\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 10: Evaluation Function\n",
        "# ---------------------------\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Convert one-hot encoded labels to class indices\n",
        "    y_true_classes = np.argmax(y_test, axis=-1).flatten()\n",
        "    y_pred_classes = np.argmax(y_pred, axis=-1).flatten()\n",
        "\n",
        "    # Create mask to ignore padding (class 0)\n",
        "    mask = y_true_classes != 0\n",
        "\n",
        "    # Apply mask to both true and predicted labels\n",
        "    y_true_masked = y_true_classes[mask]\n",
        "    y_pred_masked = y_pred_classes[mask]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true_masked, y_pred_masked)\n",
        "    print(f\"\\nTest Accuracy (excluding padding): {accuracy:.4f}\")\n",
        "\n",
        "    # Classification report (C=1, H=2, E=3 in our encoding)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true_masked,\n",
        "        y_pred_masked,\n",
        "        target_names=['C', 'H', 'E'],\n",
        "        labels=[1, 2, 3],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true_masked, y_pred_masked, labels=[1, 2, 3])\n",
        "\n",
        "    # Plot unnormalized confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['C', 'H', 'E'],\n",
        "                yticklabels=['C', 'H', 'E'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix (excluding padding)')\n",
        "    plt.show()\n",
        "\n",
        "    # Compute normalized confusion matrix\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Plot normalized confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=['C', 'H', 'E'],\n",
        "                yticklabels=['C', 'H', 'E'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Normalized Confusion Matrix (excluding padding)')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 11: Evaluate the Model\n",
        "# ---------------------------\n",
        "evaluate_model(model, X_test, y_test)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 12: Prediction Function\n",
        "# ---------------------------\n",
        "def predict_sequence(model, sequence):\n",
        "    original_length = len(sequence)\n",
        "    padded_sequence = sequence.ljust(max_seq_len, '0')[:max_seq_len]\n",
        "    encoded_sequence = [aa_dict.get(c, 0) for c in padded_sequence]\n",
        "    prediction = model.predict(np.array([encoded_sequence]))\n",
        "    return np.argmax(prediction[0][:original_length], axis=-1)\n",
        "\n",
        "# Example:\n",
        "# predicted_classes = predict_sequence(model, 'ACDEFG')\n",
        "# print(\"Predicted Secondary Structures:\", predicted_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebca5d09-1c1d-45fa-b3b6-e42f21dfaeb5",
      "metadata": {
        "id": "ebca5d09-1c1d-45fa-b3b6-e42f21dfaeb5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
